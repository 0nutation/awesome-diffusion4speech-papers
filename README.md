# awesome-diffusion4speech-papers
## Paper List
- [Speech Synthesis(TTS)](#Speech-Synthesis)
- [Automatic Speech Recognition(ASR)](#Automatic-Speech-Recognition)
- [Speech Enhancement](#Speech-Enhancement)
- [Voice Conversion(VC)](#Voice-Conversion)
- [Melspectorgram to Waveform(Vocoder)](#Vocoder)
- [Audio Generation](#Audio-Generation)
- [Music Generation](#Music-Generation)

### Speech Synthesis
- **Wavegrad2: Iterative refinement for text-to-speech synthesis** (21.06), Chen et al. [[pdf]](https://arxiv.org/abs/2106.09660)

- **Diff-tts: A denoising diffusion model for text-to-speech** (INTERSPEECH 2021), Jeong et al. [[pdf]](https://arxiv.org/abs/2104.01409)

- **Grad-tts: A diffusion probabilistic model for text-to-speech** (ICML 2021), Popov et al. [[pdf]](https://proceedings.mlr.press/v139/popov21a.html)

- **DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism** (21.05), Liu et al. [[pdf]](https://arxiv.org/abs/2105.02446)

- **PRIORGRAD: IMPROVING CONDITIONAL DENOISING DIFFUSION MODELS WITH DATA-DEPENDENT ADAPTIVE PRIOR** (ICLR 2022), Lee et al. [[pdf]](https://arxiv.org/pdf/2106.06406.pdf)

- **Diffgan-tts: High-fidelity and efficient text-to-speech with denoising diffusion gans** (22.01), Liu et al. [[pdf]](https://arxiv.org/abs/2201.11972)


- **BDDM: Bilateral denoising diffusion models for fast and high-quality speech synthesis** (ICLR 2022), W.Y.Lam et al. [[pdf]](https://arxiv.org/abs/2203.13508)

- **Prodiff: Progressive fast diffusion model for high-quality text-to-speech** (ACMMM 2022), Huang et al. [[pdf]](https://dl.acm.org/doi/abs/10.1145/3503161.3547855)

- **Zero-shot voice conditioning for denoising diffusion tts models** (22.06), Levkovitch et al. [[pdf]](https://arxiv.org/abs/2206.02246)

- **Fastdiff: A fast conditional diffusion model for high-quality speech synthesis** (IJCAI 2022), Huang et al. [[pdf]](https://arxiv.org/abs/2204.09934)

- **FastDiff 2: Dually Incorporating GANs into Diffusion Models for High-Quality Speech Synthesis**(22.09), Huang et al. [[pdf]](https://openreview.net/forum?id=-x5WuMO4APy) 

- **Guided-tts: A diffusion model for text-to-speech via classifier guidance** (ICML 2022), Kim et al. [[pdf]](https://proceedings.mlr.press/v162/kim22d.html)

- **Guided-tts 2: A diffusion model for high-quality adaptive text-to-speech with untranscribed data** (22.05), Kim et al. [[pdf]](https://arxiv.org/abs/2205.15370)

- **Prosody-TTS: Self-Supervised Prosody Pretraining with Latent Diffusion For Text-to-Speech** (22.09), Huang et al. [[pdf]](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=iRHBUsgAAAAJ&citation_for_view=iRHBUsgAAAAJ:MXK_kJrjxJIC)

- **GAN You Hear Me? Reclaiming Unconditional Speech Synthesis from Diffusion Models** (22.10), Baas et al. [[pdf]](https://arxiv.org/pdf/2210.05271)
 
- **EmoDiff: Intensity Controllable Emotional Text-to-Speech with Soft-Label Guidance** (22.11), Kang et al. [[pdf]](https://arxiv.org/pdf/2211.09383.pdf)

- **Grad-StyleSpeech: Any-speaker Adaptive Text-to-Speech Synthesis with Diffusion Models** (22.11), Kang et al. [[pdf]](https://arxiv.org/pdf/2211.09383)

- **NoreSpeech: Knowledge Distillation based Conditional Diffusion Model for Noise-robust Expressive TTS** (22.11), Yang et al. [[pdf]](https://arxiv.org/pdf/2211.02448)

- **ResGrad: Residual Denoising Diffusion Probabilistic Models for Text to Speech** (22.12), Chen et al. [[pdf]](https://arxiv.org/abs/2212.14518)

- **Text-to-speech synthesis based on latent variable conversion using diffusion probabilistic model and variational autoencoder** (22.12), Yasuda et al. [[pdf]](https://arxiv.org/pdf/2212.08329)

- **InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt** (23.01), Yang et al. [[pdf]](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=WNiojyAAAAAJ&sortby=pubdate&citation_for_view=WNiojyAAAAAJ:3fE2CSJIrl8C)

- **AN INVESTIGATION INTO THE ADAPTABILITY OF A DIFFUSION-BASED TTS MODEL** (23.03), Chen et al. [[pdf]](https://arxiv.org/pdf/2303.01849.pdf)

- **NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers** (23.04), Shen etal. [[pdf]](https://arxiv.org/pdf/2304.09116.pdf)

### Automatic Speech Recognition
- **TransFusion: Transcribing Speech with Multinomial Diffusion** (22.10), Baas et al. [[pdf]](https://arxiv.org/pdf/2210.07677.pdf)

### Speech Enhancement
- **Conditional diffusion probabilistic model for speech enhancement** (ICASSP 2022), Lu et al. [[pdf]](https://ieeexplore.ieee.org/abstract/document/9746901/)

- **Universal speech enhancement with score-based diffusion** (22.06), Serrà et al. [[pdf]](https://arxiv.org/abs/2206.03065)

- **Speech enhancement and dereverberation with diffusion-based generative models** (22.08), Richter et al. [[pdf]](https://arxiv.org/abs/2208.05830)

- **Cold Diffusion for Speech Enhancement** (22.11), Yen et al. [[pdf]](https://arxiv.org/abs/2211.02527)

### Voice Conversion
- **Diffsvc: A diffusion probabilistic model for singing voice conversion** (ASRU 2021), Liu et al. [[pdf]](https://ieeexplore.ieee.org/abstract/document/9688219/)

### Speech Edit
- **AUDIT: Audio Editing by Following Instructions with Latent Diffusion Models** (23.04), Wang et al. [[pdf]](https://export.arxiv.org/pdf/2304.00830)

### Audio generation
- **Diffsound: Discrete diffusion model for text-to-sound generation** (22.07), Yang et al. [[pdf]](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=WNiojyAAAAAJ&sortby=pubdate&citation_for_view=WNiojyAAAAAJ:roLk4NBRz8UC)

- **AudioLDM: Text-to-Audio Generation with Latent Diffusion Models** (23.01), Liu et al. [[pdf]](https://arxiv.org/pdf/2301.12503)

- **Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models** (23.01), Huang et al. [[pdf]](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=WNiojyAAAAAJ&sortby=pubdate&citation_for_view=WNiojyAAAAAJ:MXK_kJrjxJIC)

### Music generation
- **Moûsai: Text-to-Music Generation with Long-Context Latent Diffusion** (23.01), Schneider et al. [[pdf]](https://arxiv.org/pdf/2301.11757)

- **Noise2Music: Text-conditioned Music Generation with Diffusion Models** (23.01), Huang et al. [[pdf]](https://arxiv.org/pdf/2302.03917)

- **ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models** (23.02), Zhu et al. [[pdf]](https://arxiv.org/abs/2302.04456)
