# awesome-diffusion4speech-papers
## Paper List
- [Speech Synthesis(TTS)](#Speech-Synthesis)
- [Automatic Speech Recognition(ASR)](#Automatic-Speech-Recognition)
- [Speech Enhancement](#Speech-Enhancement)
- [Voice Conversion(VC)](#Voice-Conversion)
- [Melspectorgram to Waveform(Vocoder)](#Vocoder)
- [Audio Generation](#Audio-Generation)
- [Music Generation](#Music-Generation)

### Speech Synthesis
- **Wavegrad2: Iterative refinement for text-to-speech synthesis**(21.06), Chen et al. [[pdf]](https://arxiv.org/abs/2106.09660)

- **Diff-tts: A denoising diffusion model for text-to-speech**(INTERSPEECH 2021), Jeong et al. [[pdf]](https://arxiv.org/abs/2104.01409)

- **Grad-tts: A diffusion probabilistic model for text-to-speech**(ICML2021), Popov et al. [[pdf]](https://proceedings.mlr.press/v139/popov21a.html)

- **DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism**(21.05), Liu et al. [[pdf]](https://arxiv.org/abs/2105.02446)

- **Diffgan-tts: High-fidelity and efficient text-to-speech with denoising diffusion gans**(22.01), Liu et al. [[pdf]](https://arxiv.org/abs/2201.11972)

- **Prodiff: Progressive fast diffusion model for high-quality text-to-speech**(ACMMM 2022), Huang et al. [[pdf]](https://dl.acm.org/doi/abs/10.1145/3503161.3547855)

- **Zero-shot voice conditioning for denoising diffusion tts models**(22.06), Levkovitch et al. [[pdf]](https://arxiv.org/abs/2206.02246)

- **Fastdiff: A fast conditional diffusion model for high-quality speech synthesis** (IJCAI 2022), Huang et al. [[pdf]](https://arxiv.org/abs/2204.09934)

- **FastDiff 2: Dually Incorporating GANs into Diffusion Models for High-Quality Speech Synthesis**(22.09), Huang et al. [[pdf]](https://openreview.net/forum?id=-x5WuMO4APy) 

- **Guided-tts: A diffusion model for text-to-speech via classifier guidance** (ICML2022), Kim et al. [[pdf]](https://proceedings.mlr.press/v162/kim22d.html)

- **Guided-tts 2: A diffusion model for high-quality adaptive text-to-speech with untranscribed data**(22.05), Kim et al. [[pdf]](https://arxiv.org/abs/2205.15370)

- **Prosody-TTS: Self-Supervised Prosody Pretraining with Latent Diffusion For Text-to-Speech**[22.09], Huang et al. [[pdf]](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=iRHBUsgAAAAJ&citation_for_view=iRHBUsgAAAAJ:MXK_kJrjxJIC)

- **BDDM: Bilateral denoising diffusion models for fast and high-quality speech synthesis**(ICLR 2022), W.Y.Lam et al. [[pdf]](https://arxiv.org/abs/2203.13508)

- **ResGrad: Residual Denoising Diffusion Probabilistic Models for Text to Speech**(22.12), Chen et al. [[pdf]](https://arxiv.org/abs/2212.14518)

- **InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt**(23.01), Yang et al. [[pdf]](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=WNiojyAAAAAJ&sortby=pubdate&citation_for_view=WNiojyAAAAAJ:3fE2CSJIrl8C)
  
### Automatic Speech Recognition
- **TransFusion: Transcribing Speech with Multinomial Diffusion**(22.10), Baas et al. [[pdf]](https://arxiv.org/pdf/2210.07677.pdf)

### Speech Enhancement
- **Conditional diffusion probabilistic model for speech enhancement**(ICASSP 2022), Lu et al. [[pdf]](https://ieeexplore.ieee.org/abstract/document/9746901/)

- **Universal speech enhancement with score-based diffusion**(22.06), Serrà et al. [[pdf]](https://arxiv.org/abs/2206.03065)

- **Speech enhancement and dereverberation with diffusion-based generative models**(22.08), Richter et al. [[pdf]](https://arxiv.org/abs/2208.05830)

- **Cold Diffusion for Speech Enhancement**(22.11), Yen et al. [[pdf]](https://arxiv.org/abs/2211.02527)

### Voice Conversion
- **Diffsvc: A diffusion probabilistic model for singing voice conversion**(ASRU2021), Liu et al. [[pdf]](https://ieeexplore.ieee.org/abstract/document/9688219/)

### Audio generation
- **Diffsound: Discrete diffusion model for text-to-sound generation**(22.07), Yang et al. [[pdf]](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=WNiojyAAAAAJ&sortby=pubdate&citation_for_view=WNiojyAAAAAJ:roLk4NBRz8UC)

- **AudioLDM: Text-to-Audio Generation with Latent Diffusion Models**(23.01), Liu et al. [[pdf]](https://arxiv.org/pdf/2301.12503)

- **Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models**(23.01), Huang et al. [[pdf]](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=WNiojyAAAAAJ&sortby=pubdate&citation_for_view=WNiojyAAAAAJ:MXK_kJrjxJIC)

### Music generation
- **Moûsai: Text-to-Music Generation with Long-Context Latent Diffusion**(23.01), Schneider et al. [[pdf]](https://arxiv.org/pdf/2301.11757)

- **Noise2Music: Text-conditioned Music Generation with Diffusion Models**(23.01), Huang et al. [[pdf]](https://arxiv.org/pdf/2302.03917)

- **ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models**(23.02), Zhu et al. [[pdf]](https://arxiv.org/abs/2302.04456)
